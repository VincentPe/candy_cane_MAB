{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: progressively favour exploitation vs exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting random_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/random_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "# observation.reward\n",
    "# observation.lastActions\n",
    "# observation.step\n",
    "# observation.remainingOverageTime\n",
    "# observation.agentIndex\n",
    "\n",
    "# configuration.banditCount\n",
    "# configuration.episodeSteps\n",
    "# configuration.actTimeout\n",
    "# configuration.runTimeout\n",
    "# configuration.decayRate\n",
    "# configuration.sampleResolution\n",
    "\n",
    "def random_agent(observation, configuration):\n",
    "    \n",
    "#     print(f'observation contains: {observation}. '\n",
    "#           f'configuration contains: {configuration}')\n",
    "    \n",
    "    return random.randrange(configuration.banditCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting greedy_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/greedy_agent.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initiate variables\n",
    "initial_q_values = 0.5\n",
    "step_size = 0.25\n",
    "\n",
    "total_reward = 0\n",
    "last_action = None\n",
    "q_values = []\n",
    "arm_counts = []\n",
    "\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        if q_values[i] > top_value:\n",
    "            top_value = q_values[i]\n",
    "            ties = [i]\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        elif q_values[i] == top_value:\n",
    "            ties.append(i)\n",
    "    # return a random selection from ties. \n",
    "    return np.random.choice(ties)\n",
    "\n",
    "\n",
    "def greedy_agent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Takes one step for the agent. It takes in a reward and observation and \n",
    "    returns the action the agent chooses at that time step.\n",
    "    \"\"\"\n",
    "    global q_values, arm_counts, last_action, total_reward\n",
    "    \n",
    "    #print(f'Received reward {observation.reward - total_reward} for action {last_action}.')\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        q_values = [initial_q_values] * configuration.banditCount\n",
    "        arm_counts = [0] * configuration.banditCount\n",
    "    else:\n",
    "        # Get the latest reward\n",
    "        reward = observation.reward - total_reward\n",
    "        total_reward = observation.reward\n",
    "        \n",
    "        # Using a stepsize learning rate:\n",
    "        q_values[last_action] += step_size * (reward - q_values[last_action])\n",
    "        # Using a sample average: 1/arm_counts[last_action] \n",
    "        \n",
    "        q_values[last_action] = q_values[last_action] * configuration.decayRate\n",
    "        \n",
    "    last_action = int(argmax(q_values))\n",
    "    #print(f'Taking action: {last_action}. Tried {arm_counts[last_action]} times before, with q_value: {q_values[last_action]}.')\n",
    "    \n",
    "    arm_counts[last_action] += 1\n",
    "    \n",
    "    if observation.step == configuration.episodeSteps-2:\n",
    "        print(f'Total reward earned: {total_reward}')\n",
    "        arm_pulls = pd.DataFrame({'bandit': range(configuration.banditCount),\n",
    "                                  'arm_pulls': arm_counts,\n",
    "                                  'q_value': q_values})\n",
    "        print(arm_pulls.sort_values(by='arm_pulls', ascending=False))\n",
    "    \n",
    "    return last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting epsilon_greedy_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/epsilon_greedy_agent.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initiate variables\n",
    "initial_q_values = 0.5\n",
    "step_size = 0.25\n",
    "epsilon = 0.1\n",
    "\n",
    "total_reward = 0\n",
    "last_action = None\n",
    "q_values = []\n",
    "arm_counts = []\n",
    "\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        if q_values[i] > top_value:\n",
    "            top_value = q_values[i]\n",
    "            ties = [i]\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        elif q_values[i] == top_value:\n",
    "            ties.append(i)\n",
    "    # return a random selection from ties. \n",
    "    return np.random.choice(ties)\n",
    "\n",
    "\n",
    "def epsilon_greedy_agent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Takes one step for the agent. It takes in a reward and observation and \n",
    "    returns the action the agent chooses at that time step.\n",
    "    \"\"\"\n",
    "    global q_values, arm_counts, last_action, total_reward\n",
    "    \n",
    "    #print(f'Received reward {observation.reward - total_reward} for action {last_action}.')\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        q_values = [initial_q_values] * configuration.banditCount\n",
    "        arm_counts = [0] * configuration.banditCount\n",
    "    else:\n",
    "        # Get the latest reward\n",
    "        reward = observation.reward - total_reward\n",
    "        total_reward = observation.reward\n",
    "        \n",
    "        # Using a stepsize learning rate:\n",
    "        q_values[last_action] += step_size * (reward - q_values[last_action])\n",
    "        # Using a sample average: \n",
    "        # q_values[last_action] += 1/arm_counts[last_action] * (reward - q_values[last_action])\n",
    "        \n",
    "        q_values[last_action] = q_values[last_action] * configuration.decayRate\n",
    "        \n",
    "    if np.random.random() < epsilon:\n",
    "        last_action = np.random.randint(len(q_values))\n",
    "        #print(f'Taking action: {last_action} randomly.')\n",
    "    else:\n",
    "        last_action = int(argmax(q_values))\n",
    "        #print(f'Taking action: {last_action}. Tried {arm_counts[last_action]} times before, '\n",
    "        #      f'with q_value: {q_values[last_action]}.')\n",
    "    \n",
    "    arm_counts[last_action] += 1\n",
    "    \n",
    "    if observation.step == configuration.episodeSteps-2:\n",
    "        print(f'Total reward earned: {total_reward}')\n",
    "        arm_pulls = pd.DataFrame({'bandit': range(configuration.banditCount),\n",
    "                                  'arm_pulls': arm_counts,\n",
    "                                  'q_value': q_values})\n",
    "        print(arm_pulls.sort_values(by='arm_pulls', ascending=False))\n",
    "    \n",
    "    return last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting thompson_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/thompson_agent.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initiate variables\n",
    "initial_alpha = 1.  # total reward from bandit\n",
    "initial_beta = 1.  # total losses from bandit\n",
    "\n",
    "total_reward = 0\n",
    "last_action = None\n",
    "arm_counts = []\n",
    "alphas = []\n",
    "betas = []\n",
    "\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        if q_values[i] > top_value:\n",
    "            top_value = q_values[i]\n",
    "            ties = [i]\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        elif q_values[i] == top_value:\n",
    "            ties.append(i)\n",
    "    # return a random selection from ties. \n",
    "    return np.random.choice(ties)\n",
    "\n",
    "\n",
    "def thompson_agent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Takes one step for the agent. It takes in a reward and observation and \n",
    "    returns the action the agent chooses at that time step based on bayesian sample.\n",
    "    \"\"\"\n",
    "    global total_reward, last_action, arm_counts, alphas, betas\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        alphas = [initial_alpha] * configuration.banditCount\n",
    "        betas = [initial_beta] * configuration.banditCount\n",
    "        arm_counts = [0] * configuration.banditCount\n",
    "    else:\n",
    "        reward = observation.reward - total_reward  # Get the latest reward\n",
    "        total_reward = observation.reward\n",
    "        \n",
    "        if reward:\n",
    "            alphas[last_action] += 1\n",
    "        else:\n",
    "            betas[last_action] += 1\n",
    "        \n",
    "        # add decay rate\n",
    "        alphas[last_action] = alphas[last_action] * configuration.decayRate\n",
    "    \n",
    "    probas = np.random.beta(alphas, betas)\n",
    "    last_action = int(argmax(probas))\n",
    "    arm_counts[last_action] += 1\n",
    "    \n",
    "    if observation.step == configuration.episodeSteps-2:\n",
    "        print(f'Total reward earned: {total_reward}')\n",
    "        arm_pulls = pd.DataFrame({'bandit': range(configuration.banditCount),\n",
    "                                  'arm_pulls': arm_counts,\n",
    "                                  'alpha': alphas,\n",
    "                                  'beta': betas})\n",
    "        print(arm_pulls.sort_values(by='arm_pulls', ascending=False))\n",
    "    \n",
    "    return last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing thompson_agent2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agents/thompson_agent2.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initiate variables\n",
    "initial_alpha = 1.  # total reward from bandit\n",
    "initial_beta = 1.  # total losses from bandit\n",
    "\n",
    "total_reward = 0\n",
    "last_action = None\n",
    "arm_counts = []\n",
    "alphas = []\n",
    "betas = []\n",
    "\n",
    "\n",
    "def argmax(q_values):\n",
    "    \"\"\"\n",
    "    Takes in a list of q_values and returns the index of the item \n",
    "    with the highest value. Breaks ties randomly.\n",
    "    returns: int - the index of the highest value in q_values\n",
    "    \"\"\"\n",
    "    top_value = float(\"-inf\")\n",
    "    ties = []\n",
    "    \n",
    "    for i in range(len(q_values)):\n",
    "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
    "        if q_values[i] > top_value:\n",
    "            top_value = q_values[i]\n",
    "            ties = [i]\n",
    "        # if a value is equal to top value add the index to ties\n",
    "        elif q_values[i] == top_value:\n",
    "            ties.append(i)\n",
    "    # return a random selection from ties. \n",
    "    return np.random.choice(ties)\n",
    "\n",
    "\n",
    "def thompson_agent(observation, configuration):\n",
    "    \"\"\"\n",
    "    Takes one step for the agent. It takes in a reward and observation and \n",
    "    returns the action the agent chooses at that time step based on bayesian sample.\n",
    "    \"\"\"\n",
    "    global total_reward, last_action, arm_counts, alphas, betas\n",
    "    \n",
    "    if observation.step == 0:\n",
    "        alphas = [initial_alpha] * configuration.banditCount\n",
    "        betas = [initial_beta] * configuration.banditCount\n",
    "        arm_counts = [0] * configuration.banditCount\n",
    "    else:\n",
    "        reward = observation.reward - total_reward  # Get the latest reward\n",
    "        total_reward = observation.reward\n",
    "        \n",
    "        if reward:\n",
    "            alphas[last_action] += 1\n",
    "        else:\n",
    "            betas[last_action] += 1\n",
    "        \n",
    "        # add decay rate for both own and competitors action\n",
    "        for prev_action in observation.lastActions:\n",
    "            alphas[prev_action] = alphas[prev_action] * configuration.decayRate\n",
    "    \n",
    "    probas = np.random.beta(alphas, betas)\n",
    "    last_action = int(argmax(probas))\n",
    "    arm_counts[last_action] += 1\n",
    "    \n",
    "    if observation.step == configuration.episodeSteps-2:\n",
    "        print(f'Total reward earned: {total_reward}')\n",
    "        arm_pulls = pd.DataFrame({'bandit': range(configuration.banditCount),\n",
    "                                  'arm_pulls': arm_counts,\n",
    "                                  'alpha': alphas,\n",
    "                                  'beta': betas})\n",
    "        print(arm_pulls.sort_values(by='arm_pulls', ascending=False))\n",
    "    \n",
    "    return last_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arm count: 0, q_value: 1.0.\n",
      "Got a reward!\n",
      "Arm count: 1, q_value: 0.97.\n",
      "No luck this time.\n",
      "Arm count: 2, q_value: 0.84681.\n",
      "Got a reward!\n",
      "Arm count: 3, q_value: 0.8362651299999999.\n",
      "Got a reward!\n",
      "Arm count: 4, q_value: 0.8270594584899998.\n",
      "Got a reward!\n",
      "Arm count: 5, q_value: 0.8190229072617699.\n",
      "No luck this time.\n",
      "Arm count: 6, q_value: 0.7150069980395249.\n",
      "No luck this time.\n",
      "Arm count: 7, q_value: 0.6242011092885053.\n",
      "Got a reward!\n",
      "Arm count: 8, q_value: 0.6419275684088651.\n",
      "Got a reward!\n",
      "Arm count: 9, q_value: 0.6574027672209393.\n",
      "No luck this time.\n",
      "Arm count: 10, q_value: 0.57391261578388.\n"
     ]
    }
   ],
   "source": [
    "# Test stepsize\n",
    "arm_count = 0\n",
    "q_value = 1.0\n",
    "step_size = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Arm count: {arm_count}, q_value: {q_value}.')\n",
    "    arm_count += 1\n",
    "    reward = np.random.rand() < 0.75\n",
    "    if reward: \n",
    "        print('Got a reward!')\n",
    "    else:\n",
    "        print('No luck this time.')\n",
    "    q_value += step_size * (reward - q_value) \n",
    "    q_value = q_value * 0.97\n",
    "    \n",
    "print(f'Arm count: {arm_count}, q_value: {q_value}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with beta distribution\n",
    "alphas = [1,1,3]\n",
    "betas = [1, 3, 10]\n",
    "\n",
    "argmax(np.random.beta(alphas, betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "candy_cane",
   "language": "python",
   "name": "candy_cane"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
